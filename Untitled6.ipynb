{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPkGRBijdEa/u2Icn1mkwBO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zera888/tibaml1027/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy5BSzFcy6Vj"
      },
      "source": [
        "# 為了美觀 我把Future Warning關閉\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGZnMUWuzFuN"
      },
      "source": [
        "# 把 zip 解壓縮到 data 資料夾\n",
        "import zipfile\n",
        "import os\n",
        "if not os.path.exists(\"data\"):\n",
        "    f = zipfile.ZipFile(\"./pttsmalldata.zip\")\n",
        "    f.extractall(\"./data\")\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLaWPextzJ-m"
      },
      "source": [
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "# 拿取所有 *.json 檔案\n",
        "fs = glob.glob(\"./data/pttsmalldata/*.json\")\n",
        "# 我們只取用 內文 標題 和 型態(問卦...等等)\n",
        "# 但實際上我們只使用 內文 做訓練而已\n",
        "contents = []\n",
        "titles = []\n",
        "types = []\n",
        "for fn in fs:\n",
        "    with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "        contents.append(data[\"post_content\"])\n",
        "        titles.append(data[\"post_title\"])\n",
        "        types.append(data[\"post_type\"])\n",
        "df = pd.DataFrame({\n",
        "    \"type\":types,\n",
        "    \"title\":titles,\n",
        "    \"content\":contents\n",
        "}, columns=[\"type\", \"title\", \"content\"])       \n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUBP4dnWzNzH"
      },
      "source": [
        "# 由於訓練會花超級久時間, 挑選前十篇進行訓練\n",
        "df = df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJJWSyEzzRt4"
      },
      "source": [
        "# 使用 jieba 進行分詞\n",
        "import jieba\n",
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "# 使用大型字典\n",
        "big_dict_path = \"dict.txt.big\"\n",
        "if not os.path.exists(big_dict_path):\n",
        "    print(\"下載大型字典\")\n",
        "    url = \"https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big\"\n",
        "    urlretrieve(url, big_dict_path)\n",
        "jieba.set_dictionary(big_dict_path)\n",
        "\n",
        "# 需加入一些鄉民常用字彙\n",
        "ptt_dict_path = \"ptt_dic.txt\"\n",
        "if os.path.exists(ptt_dict_path):\n",
        "    print(\"載入ptt專用詞典\")\n",
        "    jieba.load_userdict(ptt_dict_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZG7FAoLzVRp"
      },
      "source": [
        "# 將標點符號去掉\n",
        "punct = set(u''':!),.:;?]}¢'\"、。〉》」』】〕〗〞︰︱︳﹐､﹒﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠々‖•·ˇˉ―--′’”([{£¥'\"‵〈《「『【〔〖（［｛￡"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAZZggpXzfoD"
      },
      "source": [
        "import re\n",
        "content = df.iloc[0][\"content\"]\n",
        "# 去掉網址 ptt的文章內容基本上都會換行 我們順便把最後的換行字元去掉\n",
        "content = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', content)\n",
        "# 使用 filter 去掉標點符號\n",
        "content = \" \".join(filter(lambda x: x not in punct, jieba.cut(content)))\n",
        "# 去掉換行符號\n",
        "content = content.replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
        "content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XArluMarzgv-"
      },
      "source": [
        "# 對表格的每一筆都做出轉換\n",
        "def process(content):\n",
        "    content = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', content)\n",
        "    content = \"\".join(filter(lambda x: x not in punct, content))\n",
        "    content = (\" \".join(jieba.cut(content))\n",
        "                  .replace(\"\\n\", \"\")\n",
        "                  .replace(\"\\r\", \"\"))\n",
        "    return content\n",
        "content_cut = df[\"content\"].apply(process)\n",
        "content_cut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZsrMkrrzkVA"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(13)\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvvsZ57zznjx"
      },
      "source": [
        "# 太少字的句子我們去掉, 因為這樣無法移動\n",
        "corpus = [sentence for sentence in content_cut if sentence.count(' ') >= 2]\n",
        "# 將每一個詞轉成數字\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "corpus = tokenizer.texts_to_sequences(corpus)\n",
        "# 統計訓練資料有多少個詞\n",
        "nb_samples = sum(len(s) for s in corpus)\n",
        "# 統計我們有多少種詞\n",
        "V = len(tokenizer.word_index) + 1\n",
        "# 降維成100\n",
        "dim = 100\n",
        "# 上文取2 下文取2\n",
        "window_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc8k_b9yzq1v"
      },
      "source": [
        "def generate_data(corpus, window_size, V):\n",
        "    # 拿最多 上文(2) + 下文(2)\n",
        "    maxlen = window_size * 2\n",
        "    # 拿出每一句\n",
        "    for words in corpus:\n",
        "        L = len(words)\n",
        "        # 拿出每一個單字\n",
        "        for index, word in enumerate(words):\n",
        "            # contexts: [上文, 下文]\n",
        "            # labels: 目標\n",
        "            contexts = []\n",
        "            labels   = []         \n",
        "            # 上文的最前面座號\n",
        "            s = index - window_size\n",
        "            # 下文的最後面座號 \n",
        "            e = index + window_size\n",
        "            # range記得要多加1\n",
        "            contexts.append([words[i] for i in range(s, e + 1) \n",
        "                                      if 0 <= i < L and i != index])\n",
        "            labels.append(word)\n",
        "            # 少於四個的前面補上0\n",
        "            x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
        "            # 答案做出one-hot encoding\n",
        "            y = np_utils.to_categorical(labels, V)\n",
        "            yield (x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2VGqovyzuNz"
      },
      "source": [
        "cbow = Sequential()\n",
        "# 使用嵌入層來得到轉化過的語意\n",
        "cbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
        "# 針對4個詞的100語意做平均\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
        "# 輸出判斷\n",
        "cbow.add(Dense(V, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtQNqPmQzxDj"
      },
      "source": [
        "cbow.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmnmdl1Gzz0v"
      },
      "source": [
        "# 訓練十個epochs\n",
        "for i in range(10):\n",
        "    loss = 0.\n",
        "    for x, y in generate_data(corpus, window_size, V):\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "    print(\"-\" * 15, \"Iteration\", i, \"-\" * 15)\n",
        "    print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYrKIkvLz2Tf"
      },
      "source": [
        "# 準備寫入我們的向量\n",
        "f = open('vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(V-1, dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nla7V8DTz5FG"
      },
      "source": [
        "vectors = cbow.get_weights()[0]\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    # 存擋的時候必須是 2 3 1 這樣空白鍵在中間\n",
        "    # 但是在用join時裡面必須是字串, 所以先用map轉換成字串\n",
        "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
        "    f.write('{} {}\\n'.format(word, str_vec))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcmPSMb-z755"
      },
      "source": [
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfq_e8IBz-3-"
      },
      "source": [
        "w2v.most_similar(\"版主\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suCSIBFu0Db6"
      },
      "source": [
        "w2v.most_similar(\"台北\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}